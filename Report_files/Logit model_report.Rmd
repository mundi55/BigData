---
title: "Untitled"
output: html_document
date: "2025-01-13"
header-includes:
- \setbeamerfont{verbatim}{size=\tiny}
---


```{r load-data, include=FALSE}
load("allgreI_filtered.RData")
```


```{r, include=FALSE}
library(ggplot2)     # For data visualization
library(dplyr)       # For data manipulation (e.g., mutate, rename)
library(tidyr)       # For data tidying (e.g., handling missing values)
library(skimr) 
```




```{r, include=FALSE}
logit_model1 <- glm(immobil ~ dispovp + age_grouped, data = allgreI_filtered, family = binomial)
summary(logit_model1)
```

```{r, include=FALSE}
logit_model2 <- glm(immobil ~ W, data = allgreI_filtered, family = binomial)
summary(logit_model2)
```

```{r, include=FALSE}
# Lijst met alle variabelen voor aparte logistische modellen
variables <- c("dispovp", "age_grouped", "sexe", "csp_grouped", "has_car", 
               "TYPE_HAB", "parking_diff", "OCCU1_grouped", "travdom", "W", "fullygrouped", "retrait")

# Loop door elke variabele en maak een apart logistisch model
for (var in variables) {
  # Dynamische naam voor het model
  model_name <- paste0("logit_model_", var)
  
  # Formule voor het model
  formula <- as.formula(paste("immobil ~", var))
  
  # Bouw het logistisch regressiemodel
  assign(model_name, glm(formula, data = allgreI_filtered, family = binomial))
  
  # Optioneel: toon de samenvatting van elk model
  cat("\n\n Samenvatting van", model_name, ":\n")
  print(summary(get(model_name)))
}

```
\tiny
```{r, include=FALSE}
# Logistisch regressiemodel
logit_model2 <- glm(immobil ~ W + dispovp + age_grouped, data = allgreI_filtered, family = binomial)
summary(logit_model2)

```



```{r, include=FALSE}
logit_model3 <- glm(immobil ~ dispovp + age_grouped + sexe + csp_grouped+ has_car + TYPE_HAB + parking_diff + OCCU1_grouped + W + fullygrouped + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model3)
```


```{r, include=FALSE}
logit_model4 <- glm(immobil ~ age_grouped + sexe + csp_grouped+ has_car + TYPE_HAB + parking_diff + OCCU1_grouped + W + fullygrouped + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model4)
```
```{r, include=FALSE}
logit_model5 <- glm(immobil ~ age_grouped + csp_grouped+ has_car + TYPE_HAB + parking_diff + OCCU1_grouped + W + fullygrouped + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model5)
```
```{r, include=FALSE}
logit_model6 <- glm(immobil ~ age_grouped + csp_grouped+ has_car + TYPE_HAB + parking_diff + OCCU1_grouped + W + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model6)
```
```{r, include=FALSE}
logit_model7 <- glm(immobil ~ age_grouped + csp_grouped+ has_car + TYPE_HAB + OCCU1_grouped + W + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model7)
```
```{r, include=FALSE}
logit_model8 <- glm(immobil ~ age_grouped + csp_grouped+ has_car + TYPE_HAB + OCCU1_grouped + W + retrait, data = allgreI_filtered, family = binomial)
summary(logit_model8)
```
```{r, include=FALSE}
logit_model9 <- glm(immobil ~ age_grouped + csp_grouped+ has_car + TYPE_HAB + OCCU1_grouped + W, data = allgreI_filtered, family = binomial)
summary(logit_model9)
```


\tiny



```{r, echo = FALSE}
# Zorg ervoor dat de immobil-variabele als factor wordt behandeld
allgreI_filtered$immobil <- factor(allgreI_filtered$immobil, levels = c("No", "Yes"))

# Maak het logistische regressiemodel op de volledige dataset
logit_model <- glm(immobil ~ age_grouped + csp_grouped + has_car + TYPE_HAB + OCCU1_grouped + W,
                   data = allgreI_filtered,
                   family = binomial)

# Samenvatting van het model
summary(logit_model)

# Bereken McFadden's R-squared
null_model <- glm(immobil ~ 1, data = allgreI_filtered, family = binomial) # Null model
logit_model_logLik <- logLik(logit_model)
null_model_logLik <- logLik(null_model)

mcfadden_r2 <- 1 - (as.numeric(logit_model_logLik) / as.numeric(null_model_logLik))
cat("McFadden's R-squared:", mcfadden_r2, "\n")

# Bereken odds ratio's en betrouwbaarheidsintervallen
odds_ratios <- exp(coef(logit_model)) # Odds ratio's
conf_int <- exp(confint(logit_model)) # Betrouwbaarheidsintervallen (95%)

# Combineer de resultaten in een dataframe
results <- data.frame(
  Variable = names(odds_ratios),
  Odds_Ratio = odds_ratios,
  CI_Lower = conf_int[, 1],
  CI_Upper = conf_int[, 2]
)

# Resultaten tonen
print("Odds Ratios met 95% betrouwbaarheidsintervallen:")
print(results)


```
Now the predictive power of the logit model will be investigated. First, a benchmark model is defined to see how much the model improve upon this.
```{r, echo = FALSE}
library(dplyr)

# Splits de data in 70% training en 30% test
set.seed(123) # Voor reproduceerbaarheid
train_indices <- sample(1:nrow(allgreI_filtered), size = 0.7 * nrow(allgreI_filtered))
train_data <- allgreI_filtered[train_indices, ]
test_data <- allgreI_filtered[-train_indices, ]

# Zorg ervoor dat de immobil-variabele als factor wordt behandeld
train_data$immobil <- factor(train_data$immobil, levels = c("No", "Yes"))
test_data$immobil <- factor(test_data$immobil, levels = c("No", "Yes"))
# Benchmarkmodel: Altijd voorspellen "No"
test_data$benchmark_predicted_class <- factor(rep("No", nrow(test_data)), levels = c("No", "Yes"))

# Confusiematrix voor het benchmarkmodel
benchmark_confusion_matrix <- table(Predicted = test_data$benchmark_predicted_class, Actual = test_data$immobil)

# Bereken evaluatiemetrics voor het benchmarkmodel
benchmark_TP <- benchmark_confusion_matrix["Yes", "Yes"]
benchmark_FP <- benchmark_confusion_matrix["Yes", "No"]
benchmark_FN <- benchmark_confusion_matrix["No", "Yes"]
benchmark_TN <- benchmark_confusion_matrix["No", "No"]

# Benchmark nauwkeurigheid
benchmark_accuracy <- (benchmark_TP + benchmark_TN) / sum(benchmark_confusion_matrix)

# Recall, Precision en F1-Score voor het benchmarkmodel
benchmark_recall <- benchmark_TP / (benchmark_TP + benchmark_FN)
benchmark_precision <- benchmark_TP / (benchmark_TP + benchmark_FP)
benchmark_f1_score <- 2 * (benchmark_precision * benchmark_recall) / (benchmark_precision + benchmark_recall)

# Resultaten tonen
cat("Benchmarkmodel results:\n")
cat("Accuracy on testdata:", benchmark_accuracy, "\n")
cat("Recall:", benchmark_recall, "\n")
cat("Precision:", benchmark_precision, "\n")
cat("F1-Score:", benchmark_f1_score, "\n")

```
For the logit model, a cut-off value of 0.35 is chosen, since the immobil variable is unbalanced. While this might decrease the accuracy, this will improve the recall. Even with this low cut-off value, the recall is still very low at 0.44. If the cut-off value is lowered further, the accuracy would decrease even further. If a cut-off value of 0.5 would be chosen, the accuracy would increase up to 0.89, but the recall would decrease to 0.24 with a precision of 0.61. This means that the model would be correct more often if it predicts immobile, but it barely does so. Since it is more important to correctly determine true immobile people, the cut-off value with higher recall is chosen.
From the dispovp odd ratio we can see that the odds of someone being immobile decreases with around 53% with each extra car they have.
```{r, echo = FALSE}

# Maak het logistische regressiemodel op de trainingsdata
logit_model <- glm(immobil ~ age_grouped + dispovp + W,
                   data = train_data,
                   family = binomial)

# Samenvatting van het model
summary(logit_model)

# Bereken odds ratio's
odds_ratios <- exp(coef(logit_model)) # Odds ratio's

# Resultaten tonen
results <- data.frame(
  Coefficient = names(odds_ratios),
  Odds_Ratio = odds_ratios
)

print("Odds Ratios:")
print(results)

# Maak voorspellingen op de testdata
test_data$predicted_prob <- predict(logit_model, newdata = test_data, type = "response")

# Voeg een voorspelde klasse toe (cut-off = 0.4)
test_data$predicted_class <- factor(ifelse(test_data$predicted_prob > 0.35, "Yes", "No"), levels = c("No", "Yes"))

# Confusiematrix voor het logistische model
confusion_matrix <- table(Predicted = test_data$predicted_class, Actual = test_data$immobil)
print(confusion_matrix)

# Bereken evaluatiemetrics
TP <- confusion_matrix["Yes", "Yes"] # True Positives
FP <- confusion_matrix["Yes", "No"]  # False Positives
FN <- confusion_matrix["No", "Yes"]  # False Negatives
TN <- confusion_matrix["No", "No"]   # True Negatives

# Nauwkeurigheid (Accuracy)
accuracy <- (TP + TN) / sum(confusion_matrix)
cat("Accuracy op testdata (Logistisch model):", accuracy, "\n")

# Recall (Sensitiviteit)
recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")

# Precision (Precisie)
precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")

# F1-Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1-Score:", f1_score, "\n")

```

